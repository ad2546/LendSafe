{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# LendSafe: Fine-tune Granite Model on Google Colab\n\nThis notebook fine-tunes IBM Granite 3.1 3B for loan explanation generation.\n\n**‚ö° OPTIMIZED FOR T4 GPU (15GB)**\n\n**Setup:**\n1. Runtime ‚Üí Change runtime type ‚Üí GPU (T4)\n2. Run all cells in order\n3. Training takes 20-40 minutes\n4. Download the fine-tuned model at the end\n\n**Memory Optimizations:**\n- Batch size: 1 (effective: 8 with gradient accumulation)\n- Sequence length: 256 tokens\n- LoRA rank: 8 (smaller adapters)\n- Gradient checkpointing enabled\n- FP16 mixed precision training\n\n**If you still get OOM errors:**\n- Reduce MAX_LENGTH to 128 in Cell 6\n- Or use A100 GPU (Colab Pro: $9.99/mo)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q torch transformers accelerate peft datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Upload Training Data\n",
    "\n",
    "Upload your `training_examples.jsonl` file from the LendSafe project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "import os\n",
    "\n",
    "print(\"üì§ Upload your training_examples.jsonl file\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "# Verify upload\n",
    "if 'training_examples.jsonl' in uploaded:\n",
    "    print(\"‚úÖ Training data uploaded successfully!\")\n",
    "    print(f\"   File size: {len(uploaded['training_examples.jsonl']) / 1024:.1f} KB\")\n",
    "else:\n",
    "    print(\"‚ùå Please upload training_examples.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Model and Configure LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer, DataCollatorForLanguageModeling\nfrom peft import LoraConfig, get_peft_model\nfrom datasets import load_dataset\n\nprint(\"üîß Configuration - Optimized for T4 GPU (15GB)\")\nMODEL_ID = \"ibm-granite/granite-3.1-3b-a800m-instruct\"\nMAX_LENGTH = 256  # Reduced to save memory\nBATCH_SIZE = 1    # Small batch for T4 GPU\nGRADIENT_ACCUMULATION = 8  # Effective batch size = 8\nLEARNING_RATE = 2e-4\nNUM_EPOCHS = 3\n\n# Check GPU\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"‚úÖ Using device: {device}\")\nif device == \"cuda\":\n    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n    \n# Clear GPU cache\nif device == \"cuda\":\n    torch.cuda.empty_cache()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load model and tokenizer with memory optimization\nprint(\"üì• Loading IBM Granite 3B model...\")\ntokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)\n\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n\n# Load with 8-bit quantization to save memory\nmodel = AutoModelForCausalLM.from_pretrained(\n    MODEL_ID,\n    torch_dtype=torch.float16,  # FP16 for memory efficiency\n    device_map=\"auto\",           # Automatic device placement\n    trust_remote_code=True,\n    low_cpu_mem_usage=True       # Reduce CPU memory during loading\n)\n\nprint(f\"‚úÖ Model loaded: {model.num_parameters():,} parameters\")\n\n# Clear cache again\nif torch.cuda.is_available():\n    torch.cuda.empty_cache()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Configure LoRA - smaller rank to save memory\nprint(\"üîß Configuring LoRA...\")\nlora_config = LoraConfig(\n    r=8,              # Reduced from 16 to save memory\n    lora_alpha=16,    # Reduced proportionally\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],  # Only attention layers\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\"\n)\n\nmodel = get_peft_model(model, lora_config)\n\ntrainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\ntotal = sum(p.numel() for p in model.parameters())\nprint(f\"‚úÖ LoRA configured:\")\nprint(f\"   Trainable params: {trainable:,} ({100*trainable/total:.2f}%)\")\nprint(f\"   Total params: {total:,}\")\n\n# Enable gradient checkpointing to save memory\nmodel.gradient_checkpointing_enable()\nprint(f\"‚úÖ Gradient checkpointing enabled (saves ~30% memory)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "print(\"üìä Loading training data...\")\n",
    "dataset = load_dataset('json', data_files='training_examples.jsonl', split='train')\n",
    "print(f\"‚úÖ Loaded {len(dataset)} examples\")\n",
    "\n",
    "# Format prompts\n",
    "def format_prompt(example):\n",
    "    prompt = f\"\"\"### Instruction:\n",
    "{example['instruction']}\n",
    "\n",
    "### Input:\n",
    "{example['input']}\n",
    "\n",
    "### Response:\n",
    "{example['output']}\"\"\"\n",
    "    return {\"text\": prompt}\n",
    "\n",
    "dataset = dataset.map(format_prompt, remove_columns=dataset.column_names)\n",
    "\n",
    "# Tokenize\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=MAX_LENGTH,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "tokenized_dataset = dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=[\"text\"]\n",
    ")\n",
    "\n",
    "# Split\n",
    "split_dataset = tokenized_dataset.train_test_split(test_size=0.1, seed=42)\n",
    "print(f\"‚úÖ Train: {len(split_dataset['train'])}, Val: {len(split_dataset['test'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Training arguments - Optimized for T4 GPU\ntraining_args = TrainingArguments(\n    output_dir=\"./granite-finetuned\",\n    num_train_epochs=NUM_EPOCHS,\n    per_device_train_batch_size=BATCH_SIZE,\n    per_device_eval_batch_size=BATCH_SIZE,\n    gradient_accumulation_steps=GRADIENT_ACCUMULATION,\n    learning_rate=LEARNING_RATE,\n    fp16=True,                              # Use FP16 to save memory\n    logging_steps=20,\n    eval_strategy=\"steps\",\n    eval_steps=100,\n    save_strategy=\"steps\",\n    save_steps=100,\n    save_total_limit=2,\n    warmup_steps=50,\n    load_best_model_at_end=True,\n    report_to=\"none\",\n    gradient_checkpointing=True,            # Save memory during backward pass\n    optim=\"adamw_torch\",                    # Standard optimizer\n    max_grad_norm=1.0,                      # Gradient clipping\n)\n\n# Data collator\ndata_collator = DataCollatorForLanguageModeling(\n    tokenizer=tokenizer,\n    mlm=False\n)\n\n# Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=split_dataset[\"train\"],\n    eval_dataset=split_dataset[\"test\"],\n    data_collator=data_collator,\n)\n\nprint(\"üöÄ Starting training...\")\nprint(\"‚è∞ Expected time: 20-40 minutes on T4 GPU\")\nprint(\"üí° Memory optimizations:\")\nprint(\"   - Batch size: 1 (effective: 8 with gradient accumulation)\")\nprint(\"   - Sequence length: 256 tokens\")\nprint(\"   - LoRA rank: 8 (smaller adapters)\")\nprint(\"   - Gradient checkpointing: Enabled\")\nprint(\"   - FP16 training: Enabled\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train!\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Test the Fine-tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test generation\n",
    "test_prompt = \"\"\"### Instruction:\n",
    "Explain why this loan application was approved.\n",
    "\n",
    "### Input:\n",
    "Credit Score: 720\n",
    "Debt-to-Income Ratio: 28%\n",
    "Loan Amount: $25,000\n",
    "Annual Income: $85,000\n",
    "Employment Length: 5 years\n",
    "Delinquencies (2 yrs): 0\n",
    "Credit Inquiries (6 mo): 1\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "\n",
    "inputs = tokenizer(test_prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "print(\"üß™ Testing fine-tuned model...\")\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=200,\n",
    "        temperature=0.7,\n",
    "        do_sample=True,\n",
    "        top_p=0.9,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"GENERATED EXPLANATION:\")\n",
    "print(\"=\"*60)\n",
    "print(response)\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save and Download Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "print(\"üíæ Saving fine-tuned model...\")\n",
    "trainer.save_model(\"./granite-finetuned-final\")\n",
    "tokenizer.save_pretrained(\"./granite-finetuned-final\")\n",
    "print(\"‚úÖ Model saved!\")\n",
    "\n",
    "# Create zip for download\n",
    "!zip -r granite-finetuned-final.zip granite-finetuned-final/\n",
    "print(\"\\nüì¶ Model packaged for download\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the model\n",
    "from google.colab import files\n",
    "\n",
    "print(\"‚¨áÔ∏è Downloading fine-tuned model...\")\n",
    "files.download('granite-finetuned-final.zip')\n",
    "print(\"\\n‚úÖ Download started!\")\n",
    "print(\"\\nTo use locally:\")\n",
    "print(\"1. Extract granite-finetuned-final.zip\")\n",
    "print(\"2. Move to LendSafe/models/granite-finetuned/\")\n",
    "print(\"3. Run evaluation script\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéâ Done!\n",
    "\n",
    "Your Granite model is now fine-tuned for loan explanations!\n",
    "\n",
    "**Next steps:**\n",
    "1. Download the model zip file\n",
    "2. Extract and place in your local LendSafe project\n",
    "3. Run `python scripts/evaluate_model.py` to get metrics\n",
    "\n",
    "**Training Summary:**\n",
    "- Model: IBM Granite 4.0 H 350M\n",
    "- Method: LoRA (0.1% parameters trained)\n",
    "- Data: 1,500 loan explanation examples\n",
    "- Training time: ~15-30 minutes on T4 GPU\n",
    "- Cost: $0 (free Colab)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}