`torch_dtype` is deprecated! Use `dtype` instead!
The fast path is not available because one of `(selective_state_update, causal_conv1d_fn, causal_conv1d_update)` is None. Falling back to the naive implementation. To install follow https://github.com/state-spaces/mamba/#installation and https://github.com/Dao-AILab/causal-conv1d
============================================================
GRANITE MODEL FINE-TUNING WITH LORA
============================================================
ðŸ“¥ Loading IBM Granite model...
âœ… Model loaded: 340,332,224 parameters

ðŸ”§ Configuring LoRA...
âœ… LoRA configured:
   Trainable params: 327,680 (0.10%)
   Total params: 340,659,904
   Memory savings: 99.90% parameters frozen

ðŸ“Š Preparing dataset...
âœ… Loaded 1500 examples
Map:   0%|          | 0/1500 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1500/1500 [00:00<00:00, 13955.03 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1500/1500 [00:00<00:00, 13679.72 examples/s]
âœ… Training samples: 1350
âœ… Validation samples: 150

ðŸš€ Starting fine-tuning...

â° Training will take 30-60 minutes on M2 MacBook Air...
ðŸ’¡ Monitor training loss - should decrease steadily

  0%|          | 0/255 [00:00<?, ?it/s]/Users/atharvadeshmukh/LendSafe/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.
  warnings.warn(warn_msg)
GraniteMoeHybrid requires an initialized `HybridMambaAttentionDynamicCache` to return a cache. Because one was not provided, no cache will be returned.
/Users/atharvadeshmukh/LendSafe/.venv/lib/python3.12/site-packages/torch/nn/functional.py:5294: UserWarning: MPS: The constant padding of more than 3 dimensions is not currently supported natively. It uses View Ops default implementation to run. This may have performance implications. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/mps/operations/Pad.mm:468.)
  return torch._C._nn.pad(input, pad, mode, value)
Traceback (most recent call last):
  File "/Users/atharvadeshmukh/LendSafe/scripts/finetune_granite.py", line 292, in <module>
    main()
  File "/Users/atharvadeshmukh/LendSafe/scripts/finetune_granite.py", line 276, in main
    trainer = train_model(model, tokenizer, dataset)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/atharvadeshmukh/LendSafe/scripts/finetune_granite.py", line 192, in train_model
    trainer.train()
  File "/Users/atharvadeshmukh/LendSafe/.venv/lib/python3.12/site-packages/transformers/trainer.py", line 2325, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/Users/atharvadeshmukh/LendSafe/.venv/lib/python3.12/site-packages/transformers/trainer.py", line 2674, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/atharvadeshmukh/LendSafe/.venv/lib/python3.12/site-packages/transformers/trainer.py", line 4020, in training_step
    loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/atharvadeshmukh/LendSafe/.venv/lib/python3.12/site-packages/transformers/trainer.py", line 4110, in compute_loss
    outputs = model(**inputs)
              ^^^^^^^^^^^^^^^
  File "/Users/atharvadeshmukh/LendSafe/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/atharvadeshmukh/LendSafe/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/atharvadeshmukh/LendSafe/.venv/lib/python3.12/site-packages/peft/peft_model.py", line 1850, in forward
    return self.base_model(
           ^^^^^^^^^^^^^^^^
  File "/Users/atharvadeshmukh/LendSafe/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/atharvadeshmukh/LendSafe/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/atharvadeshmukh/LendSafe/.venv/lib/python3.12/site-packages/peft/tuners/tuners_utils.py", line 222, in forward
    return self.model.forward(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/atharvadeshmukh/LendSafe/.venv/lib/python3.12/site-packages/transformers/models/granitemoehybrid/modeling_granitemoehybrid.py", line 1717, in forward
    outputs = self.model(
              ^^^^^^^^^^^
  File "/Users/atharvadeshmukh/LendSafe/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/atharvadeshmukh/LendSafe/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/atharvadeshmukh/LendSafe/.venv/lib/python3.12/site-packages/transformers/utils/generic.py", line 918, in wrapper
    output = func(self, *args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/atharvadeshmukh/LendSafe/.venv/lib/python3.12/site-packages/transformers/models/granitemoehybrid/modeling_granitemoehybrid.py", line 1389, in forward
    layer_outputs = decoder_layer(
                    ^^^^^^^^^^^^^^
  File "/Users/atharvadeshmukh/LendSafe/.venv/lib/python3.12/site-packages/transformers/modeling_layers.py", line 94, in __call__
    return super().__call__(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/atharvadeshmukh/LendSafe/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/atharvadeshmukh/LendSafe/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/atharvadeshmukh/LendSafe/.venv/lib/python3.12/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/Users/atharvadeshmukh/LendSafe/.venv/lib/python3.12/site-packages/transformers/models/granitemoehybrid/modeling_granitemoehybrid.py", line 1176, in forward
    hidden_states = self.mamba(
                    ^^^^^^^^^^^
  File "/Users/atharvadeshmukh/LendSafe/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/atharvadeshmukh/LendSafe/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/atharvadeshmukh/LendSafe/.venv/lib/python3.12/site-packages/transformers/models/granitemoehybrid/modeling_granitemoehybrid.py", line 865, in forward
    return self.torch_forward(hidden_states, cache_params, cache_position, attention_mask)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/atharvadeshmukh/LendSafe/.venv/lib/python3.12/site-packages/transformers/models/granitemoehybrid/modeling_granitemoehybrid.py", line 794, in torch_forward
    Y_diag = (M[..., None] * hidden_states[:, :, None]).sum(dim=3)
              ~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~
RuntimeError: MPS backend out of memory (MPS allocated: 8.45 GiB, other allocations: 10.73 MiB, max allowed: 9.07 GiB). Tried to allocate 768.00 MiB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).
  0%|          | 0/255 [02:43<?, ?it/s]
